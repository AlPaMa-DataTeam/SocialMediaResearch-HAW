---
title: "R Notebook"
autor: "AlPaMa-DataTeam"
output: html_notebook
---

Description about the Project

# Setup:
```{r, include=FALSE}
# Packages
packages <- c("ggplot2", "dplyr", "readr", "stringr", "purrr", "janitor", "ggrepel", "rtweet", "twitteR", 'purrr', 'tidyr', 'ggwordcloud', "raster", "leaflet", "sf", "rgeos")

# Function to check installation and install packages
pkgTest <- function(x)
{
  if (!require(x,character.only = TRUE))
  {
    install.packages(x,dep=TRUE)
    if(!require(x,character.only = TRUE)) stop("Package not found")
  }
}

# Auto load and install packages
invisible(for (package in packages) {
  pkgTest(package)
  library(package, character.only = TRUE)
})

# Twitter access
api_key <- "KB9450oyqXW9doi7Hb1HlfIfL"
api_secret_key <- "2fhDc53TtMMsHtVugsieYIQWP9T46K2wmPcc7GmgyYxqwQhiTT"
access_token <- "94161327-dWQWONF3UvNPam1fhV10fXCJnIXXinBSCriliCBfP"
access_token_secret <- "ZFA0GSjrTDyIfaASbqgK9Hk775bYkXSyHbdJ2LPtlRHmb"

## authenticate via web browser
token <- create_token(
  app = "AlPaMa - Twitter Analysis",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)
```

# Data:
## Parliamentarians 
```{r, include=FALSE}
urlfile = "https://raw.githubusercontent.com/Leibniz-HBI/DBoeS-Automatization/master/db/reviewed/Parlamentarier.csv"
mainData <- read_csv("data/ParlamentarierBackup.csv")
# Try to overwrite up latest data
try(mainData <- read_csv(url(urlfile)))
# Create Backup
write_csv(mainData, "data/ParlamentarierBackup.csv")
```
## Tweets 
```{r, include=FALSE}
# Relevant parliamentarians for Twitter scraping
twitterUser = mainData %>% 
  filter(Kategorie != "EU-Parlament") %>% # Switch between EU only and non EU by changing the != to ==
  filter(!is.na(SM_Twitter_id)) %>% 
  mutate(SM_Twitter_id = as.character(SM_Twitter_id))

# Check current rate limits
rate_limit(token) # All
#rate_limit(token, "statuses/user_timeline") # user_timeline
allTweetsRDS = 'data/tweets.rds'
if (file.exists(allTweetsRDS)){
  scrapedTweetsPerUser = readRDS(allTweetsRDS)
} else {
  scrapedTweetsPerUser <- list()
  userList = list(twitterUser$SM_Twitter_user)
  rl <- rate_limit(token, "statuses/user_timeline")
  for(user in userList[[1]]){
    scrapedTweetsPerUser[[user]] <- get_timeline(user, n = 1500, check = F)
    print("rate limit remaining:", str(rl$remaining))
    print("at user:", str(user))
    # Artifically Counting down the rate limit
    rl <- rl %>% 
      mutate(remaining = remaining - 1)
    # if rate limit exhausted, then wait to rate limit reset
    if (rl$remaining < 5L) {
      # Get Rate Limits to get time till reset
      rl <- rate_limit(token, "statuses/user_timeline")
      print("rate limit exceeded, waiting for 900s at user", str(user))
      Sys.sleep(as.numeric(rl$reset, "secs"))
      # Get fresh rate limit info after waiting
      rl <- rate_limit(token, "statuses/user_timeline")
    }
  }
  saveRDS(scrapedTweetsPerUser, file = "data/tweets.rds")
}

```
## Data processing
```{r, include=FALSE}
# All scraped tweets in a tidy dataframe. Modifications: All screen_name Data is lowercase, added column with amount of tweets per user
allScrapedTweets = data.table::rbindlist(scrapedTweetsPerUser) %>% 
  mutate(screen_name = tolower(screen_name)) %>% 
  group_by(screen_name) %>% 
  dplyr::mutate(tweetAmount = n())


# Convert every username to lowercase for correct joining
mainDataLower = mainData %>% 
  mutate(SM_Twitter_user = tolower(SM_Twitter_user))
  
# Joined the dataframes to add all politicians without tweets and set tweetsAmount to 0
mainDataWithTweets = allScrapedTweets %>% 
  full_join(mainDataLower, by = c('screen_name'='SM_Twitter_user')) %>% 
  mutate(tweetAmount = replace_na(tweetAmount, 0))

# Expanded vectors with hashtags to have one hashtag per row (rest of the data got duplicated)
expandedHashtags = mainDataWithTweets %>% 
  unnest(hashtags)
# Amount of tweets per person. Problem with 96 missing entries (not EU-Parlamentarians)
tweetAmount = mainDataWithTweets %>% 
  filter(Kategorie != "EU-Parlament") %>%
  distinct(Name, .keep_all=TRUE) %>% 
  select(Name, screen_name, tweetAmount)


# Wenn jmd Lust hat kann man hier rausfinden warum 96 Politiker fehlen. Eigentlich müsste der full_join alle in Tweet Amount eingefügt haben
mainDataLowerNames = mainDataLower %>% 
  select(Name)
  
tweetAmountNames = tweetAmount %>% 
  select(Name)

mainDataWithState = mainData %>% 
  mutate(Bundesland, Bundesland = case_when(
    Kategorie == "Abgeordnetenhaus von Berlin"~"Berlin",
    Kategorie == "Bayerischer Landtag"~"Bayern",
    Kategorie == "Bremische Bürgerschaft"~"Bremen",
    Kategorie == "Hamburgische Bürgerschaft"~"Hamburg",
    Kategorie == "Hessischer Landtag"~"Hessen",
    Kategorie == "Landtag Brandenburg"~"Brandenburg",
    Kategorie == "Landtag des Saarlandes"~"Saarland",
    Kategorie == "Landtag Mecklenburg-Vorpommern"~"Mecklenburg-Vorpommern",
    Kategorie == "Landtag Nordrhein-Westfalen"~"Nordrhein-Westfalen",
    Kategorie == "Landtag Rheinland-Pfalz"~"Rheinland-Pfalz",
    Kategorie == "Landtag Sachsen-Anhalt"~"Sachsen-Anhalt",
    Kategorie == "Landtag von Baden-Württemberg"~"Baden-Württemberg",
    Kategorie == "Niedersächsischer Landtag"~"Niedersachsen",
    Kategorie == "Sächsischer Landtag"~"Sachsen",
    Kategorie == "Schleswig-Holsteinischer Landtag"~"Schleswig-Holstein",
    Kategorie == "Thüringer Landtag"~"Thüringen",
    Kategorie == "Bundestag"~"Deutschland",
    TRUE ~ Kategorie))

topHashtags = expandedHashtags %>% 
  select(Partei, hashtags) %>% 
  filter(!is.na(hashtags)) %>% 
  group_by(Partei, hashtags) %>%
  mutate(Partei, Partei = case_when(
    Partei == "CSU"~"CDU",
    Partei == "FDP/DVP"~"FDP",
    Partei == "FW"~"Andere",
    Partei == "fraktionslos"~"Andere",
    TRUE ~ Partei
  )) %>% 
  dplyr::mutate(
    no_hashtags = n()
  ) %>% 
  distinct(Partei, hashtags, no_hashtags, .keep_all = TRUE) %>%
  filter(no_hashtags > 20)
  

```
Plots:

## Social Media usage of german political paries in 2021
```{r}
tweetTimelineData = mainDataWithTweets %>% 
  filter(created_at.x > as.POSIXct("2021-01-01")) %>% 
    mutate(Partei, Partei = case_when(
    Partei == "CSU"~"CDU",
    Partei == "FDP/DVP"~"FDP",
    Partei == "FW"~"Andere",
    Partei == "fraktionslos"~"Andere",
    Partei == "SSW"~"Andere",
    Partei == "BIW"~"Andere",
    Partei == "BVB/FW"~"Andere",
    TRUE ~ Partei
  ))
partei_farben = c(
    "CDU" = "black",
    "SPD" = "#E30013",
    "FDP" = "#FFDD00",
    "LINKE" = "#BD3075",
    "GRÜNE" = "#19A229",
    "AfD" = "#009FE1",
    "Andere" = "grey"
)

ggplot(tweetTimelineData)+
  geom_freqpoly(aes(created_at.x, color = Partei), position = "stack")+ 
  scale_x_datetime(date_breaks = "1 month")+ # Anpassen um die Zeitangabe richtig anzuzeigen
  scale_color_manual(values = partei_farben)+
  labs(
    title = "Tweetsvorkommen der Politiker",
    subtitle = "Zeitverlauf 2021",
    x = "Zeit",
    y = "Anzahl Tweets",
    color = "Retweet"
  )
```

## Word Cloud of commonly used hashtags of german german political paries in 2021
```{r}
ggplot(topHashtags, aes(label = hashtags, size = no_hashtags, color = no_hashtags))+
  geom_text_wordcloud_area(area_corr = TRUE)+
  scale_size_area(max_size = 20) +
  theme_minimal()+
  scale_color_gradient(low = "black", high = "darkblue")+
  labs(
    title = "Die 50 meist verwendeten Hashtags",
    subtitle = "Der aktivsten User zum Thema Super League"
  )+
  facet_wrap(~Partei)

```

## Twitter Usage per state in 2021 (Map test)
```{r}
# References:
# https://cran.r-project.org/web/packages/leaflet.minicharts/vignettes/introduction.html
# https://r-spatial.github.io/mapview/articles/articles/mapview_04-popups.html


de_map <- 
  getData("GADM", country="Germany", level=1, path='./data/mapfiles') %>% 
  st_as_sf() %>% 
  left_join(mainDataWithState, by = c("NAME_1" = "Bundesland"))
  # select() - waiting for dataframe


de_map$randomData <- rnorm(n=nrow(de_map), 150, 30)

pal <- colorQuantile("Reds", NULL, n = 9)
popup <- paste0("<strong>Bundesland: </strong>", de_map$NAME_1, "<br>",
                "<strong>Random data test: </strong>", round(de_map$randomData,2))

mapTest <- 
  leaflet() %>% 
  addProviderTiles(providers$CartoDB.Voyager) %>% 
  setView(lng = 10.4507147, lat = 50.9833118, zoom = 6) %>% 
  addPolygons(data = de_map, 
              fillColor= ~pal(randomData),
              fillOpacity = 0.4, 
              weight = 2, 
              color = "white",
              popup = popup)

```
