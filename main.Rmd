---
title: "R Notebook"
autor: AlPaMa-DataTeam
output:
  html_document:
    df_print: paged
---

Description about the Project

# Setup:
```{r, include=FALSE}
# Packages
packages <- c("ggplot2", "dplyr", "readr", "stringr", "purrr", "janitor", "ggrepel", "rtweet", "twitteR", 'purrr', 'tidyr', 'ggwordcloud', "raster", "leaflet", "sf", "rgeos", "plotly")

# Function to check installation and install packages
pkgTest <- function(x)
{
  if (!require(x,character.only = TRUE))
  {
    install.packages(x,dep=TRUE)
    if(!require(x,character.only = TRUE)) stop("Package not found")
  }
}

# Auto load and install packages
invisible(for (package in packages) {
  pkgTest(package)
  library(package, character.only = TRUE)
})

# Twitter access
api_key <- "KB9450oyqXW9doi7Hb1HlfIfL"
api_secret_key <- "2fhDc53TtMMsHtVugsieYIQWP9T46K2wmPcc7GmgyYxqwQhiTT"
access_token <- "94161327-dWQWONF3UvNPam1fhV10fXCJnIXXinBSCriliCBfP"
access_token_secret <- "ZFA0GSjrTDyIfaASbqgK9Hk775bYkXSyHbdJ2LPtlRHmb"

## authenticate via web browser
token <- create_token(
  app = "AlPaMa - Twitter Analysis",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)
```

# Daten:
## Parlamentarier
```{r}
urlfile = "https://raw.githubusercontent.com/Leibniz-HBI/DBoeS-Automatization/master/db/reviewed/Parlamentarier.csv"
mainData <- read_csv("data/ParlamentarierBackup.csv")
# Try to overwrite up latest data
try(mainData <- read_csv(url(urlfile)))
mainData = mainData %>% 
  mutate(SM_Twitter_user = tolower(SM_Twitter_user)) %>% 
  filter(Kategorie != "EU-Parlament") %>% 
  mutate(Kategorie, Bundesland = case_when(
    Kategorie == "Abgeordnetenhaus von Berlin"~"Berlin",
    Kategorie == "Bayerischer Landtag"~"Bayern",
    Kategorie == "Bremische Bürgerschaft"~"Bremen",
    Kategorie == "Hamburgische Bürgerschaft"~"Hamburg",
    Kategorie == "Hessischer Landtag"~"Hessen",
    Kategorie == "Landtag Brandenburg"~"Brandenburg",
    Kategorie == "Landtag des Saarlandes"~"Saarland",
    Kategorie == "Landtag Mecklenburg-Vorpommern"~"Mecklenburg-Vorpommern",
    Kategorie == "Landtag Nordrhein-Westfalen"~"Nordrhein-Westfalen",
    Kategorie == "Landtag Rheinland-Pfalz"~"Rheinland-Pfalz",
    Kategorie == "Landtag Sachsen-Anhalt"~"Sachsen-Anhalt",
    Kategorie == "Landtag von Baden-Württemberg"~"Baden-Württemberg",
    Kategorie == "Niedersächsischer Landtag"~"Niedersachsen",
    Kategorie == "Sächsischer Landtag"~"Sachsen",
    Kategorie == "Schleswig-Holsteinischer Landtag"~"Schleswig-Holstein",
    Kategorie == "Thüringer Landtag"~"Thüringen",
    Kategorie == "Bundestag"~"Deutschland",
    TRUE ~ Kategorie)) %>% 
  mutate(Partei, Partei = case_when(
    Partei == "CSU"~"CDU/CSU",
    Partei == "CDU"~"CDU/CSU",
    Partei == "FDP/DVP"~"FDP",
    Partei == "FW"~"Andere",
    Partei == "fraktionslos"~"Andere",
    Partei == "SSW"~"Andere",
    Partei == "BIW"~"Andere",
    Partei == "BVB/FW"~"Andere",
    TRUE ~ Partei
  ))
# Backup
write_csv(mainData, "data/ParlamentarierBackup.csv")
```

## Tweets 
```{r}
# Relevant parliamentarians for Twitter scraping
twitterUser = mainData %>% 
  filter(Kategorie != "EU-Parlament") %>% # Switch between EU only and non EU by changing the != to ==
  filter(!is.na(SM_Twitter_id)) %>% 
  mutate(SM_Twitter_id = as.character(SM_Twitter_id))

# Check current rate limits
#rate_limit(token) # All
#rate_limit(token, "statuses/user_timeline") # user_timeline
allTweetsRDS = 'data/tweets.rds'
if (file.exists(allTweetsRDS)){
  scrapedTweetsPerUser = readRDS(allTweetsRDS)
} else {
  scrapedTweetsPerUser <- list()
  maxTweets = 1500
  userList = list(twitterUser$SM_Twitter_user)
  rl <- rate_limit(token, "statuses/user_timeline")
  for(user in userList[[1]]){
    scrapedTweetsPerUser[[user]] <- get_timeline(user, n = maxTweets, check = F)
    lastRLCosts = ceiling(nrow(tail(scrapedTweetsPerUser, 1)[[1]])/200)+1 # rate limit costs of last operation
    print(sprintf("Finished user: %s; rate limit remaining: %i", user,rl$remaining))
    # Artifically Counting down the rate limit by calculated rate limit
    rl <- rl %>% 
      mutate(remaining = remaining - lastRLCosts) 
    # if rate calculated limit is low check real rate limit
    if (rl$remaining < lastRLCosts * 2) {
      # Get rate limits to get time till reset and real rate limit
      rl <- rate_limit(token, "statuses/user_timeline")
      if (rl$remaining < maxTweets / 200) {
        print(sprintf("rate limit exceeded, waiting for %.6g minutes", round(rl$reset, digits = 2)))
        Sys.sleep(as.numeric(rl$reset, "secs"))
        # Get fresh rate limit info after waiting
        rl <- rate_limit(token, "statuses/user_timeline")
      }
    }
  }
  saveRDS(scrapedTweetsPerUser, file = "data/tweets.rds")
}

# Optionaler Abgleich und erneutes Definieren der zu scrapenden User (Kann aktiviert werden falls das Scrapen abbricht)
'missingTwitterUsers = tibble(User = scrapedTweetsPerUser) %>% 
  mutate(tweets = map_int(User, nrow)) %>% 
  mutate(usernames = names(scrapedTweetsPerUser)) %>% 
  dplyr::select(-User) %>% 
  mutate(missing = ifelse(tweets == 0, "Fehlende Twitter Daten", "Tiwtter Daten vorhanden"))
twitterUser = subset(twitterUser, !(SM_Twitter_user %in% missingTwitterUsers$usernames))'
```

## Datenmodelierung
### Datenmodelierung gescrapten Liste der Twitter Daten (allgeimein und für das Kuchendiagramm)
```{r, warning=FALSE, echo=FALSE, message=FALSE, results='hide'}
# Umwandeln der gescrapten Liste in ein Dataframe und Verbinden mit dem Parlamentarier Dataframe
allScrapedTweets = bind_rows(scrapedTweetsPerUser) %>% 
  mutate(screen_name = tolower(screen_name)) %>% 
  group_by(screen_name) %>% 
  full_join(mainData, by = c('screen_name'='SM_Twitter_user'))

missingTwitterUsers = tibble(User = scrapedTweetsPerUser) %>% 
  mutate(tweets = map_int(User, nrow)) %>% 
  mutate(usernames = names(scrapedTweetsPerUser)) %>% 
  dplyr::select(-User) %>% 
  mutate(missing = ifelse(tweets == 0, "Fehlende Twitter Daten", "Twitter Daten vorhanden"))
# Löschen der gescrapten Liste soabld sie nicht mehr benötigt wird um RAM zu spraren
rm(scrapedTweetsPerUser)
```

### Datenmodellierung für das gestapelte Balkendiagramm
```{r, warning=FALSE, echo=FALSE, message=FALSE, results='hide'}
stackedBarData = mainData %>% 
  left_join(missingTwitterUsers, by = c("SM_Twitter_user" = "usernames" )) %>% 
  mutate(missing = replace_na(missing, "Kein Account")) %>% 
  dplyr::select(Name, Partei, missing) %>%
  group_by(Partei) %>%
  dplyr::mutate(count = 1)
```

### Datenmodellierung für den Zeitstrahl
```{r, warning=FALSE, echo=FALSE, message=FALSE, results='hide'}
tweetTimelineData = allScrapedTweets %>% 
  filter(created_at.x > as.POSIXct("2021-01-01"))
```

### Datenmodellierung für die Wordcloud (bei der Karte wiederverwendet)
```{r, warning=FALSE, echo=FALSE, message=FALSE, results='hide'} 
topHashtags = allScrapedTweets %>% 
  unnest(hashtags) %>% 
  mutate(hashtags = tolower(hashtags)) %>% 
  dplyr::select(Partei, hashtags, Bundesland) %>% 
  filter(!is.na(hashtags)) %>% 
  group_by(Partei, hashtags) %>%  
  dplyr::mutate(
    no_hashtags = n()
  ) %>% 
  distinct(Partei, hashtags, no_hashtags, .keep_all = TRUE)
topHashtagsLimit = topHashtags %>% 
  filter(no_hashtags > 50)
```

### Daten für die Karte
```{r, warning=FALSE, echo=FALSE, message=FALSE, results='hide'}
mapData = allScrapedTweets %>% 
  group_by(Bundesland) %>%
  dplyr::mutate(
    no_tweets = n()
  ) %>%
  dplyr::select(Bundesland, no_tweets) %>% 
  distinct(Bundesland, .keep_all=TRUE) %>% 
  filter(Bundesland != "Deutschland")

# Evaluieren der drei häufigsten Hashtags pro partei
top3Hashtags = topHashtags %>% 
  group_by(Bundesland, Partei) %>% 
  slice_max(order_by = no_hashtags, n = 3)

# Hinzufügen einer Spalte mit den Hashtags pro Partei in form eines HTML-Strings
hashtagHTML = top3Hashtags %>% 
  ungroup() %>% 
  dplyr::select(-screen_name) %>% 
  group_by(Bundesland, Partei) %>%
  mutate(hashtags3 = paste0("#", hashtags, collapse = ", ")) %>% 
  mutate(partei = paste0("<strong>",Partei,": </strong>")) %>% 
  distinct(Partei, Bundesland, .keep_all=TRUE) %>% 
  mutate(html = paste0(partei, hashtags3, "<br>")) %>% 
  group_by(Bundesland) %>% 
  mutate(html = paste0(html, collapse = "")) %>% 
  distinct(Bundesland, html)

# Kartendaten zusammenführen
de_map <- 
  getData("GADM", country="Germany", level=1, path='./data/mapfiles') %>% 
  st_as_sf() %>% 
  left_join(mapData, by = c("NAME_1" = "Bundesland")) %>% 
  left_join(hashtagHTML, by = c("NAME_1" = "Bundesland"))
```

## Löschen nicht benötigter Dataframes
```{r, warning=FALSE, echo=FALSE, message=FALSE, results='hide'}
rm(topHashtags, hashtagHTML, top3Hashtags,  mapData)
```


# Plots:

## Verhältnis zwischen gescrapet zu nicht scrapebaren Twitternutzern (Parlamentarier ohne angegebenen Twitter Account sind davon ausgeschlossen)
```{r, warning=FALSE, echo=FALSE, message=FALSE, results='hide'}
pieColors <- c("#FF7F0E", "#1F77B4")

plot_ly(
  missingTwitterUsers, 
  labels = ~missing, 
  type = "pie",
  textinfo = "percentage",
  textposition = "inside",
  color = list(pieColors)
  ) %>% 
  layout(title = "Anteil von vorhandenen Twitter Daten")
```

## Verhältnis ziwschen Parlamentarier jeder Partei mit vorhandenem Twitter Account, fehlendem Twitter Account und nicht scrapebaren Twitter Account pro Partei
```{r, warning=FALSE, echo=FALSE, message=FALSE, results='hide'}
options(scipen = 999)

stackedBar <- 
  ggplot(stackedBarData, aes(fill=missing, y=count, x=Partei)) +
  geom_bar(position="stack", 
           stat="identity") +
  scale_fill_manual("", values = c("#ece7f2", "#a6bddb", "#2b8cbe")) +
  ggtitle("Verteilung von Twitter Konten pro Partei") + 
  ylab("Anzahl der Konten") +
  xlab("") + 
  theme(panel.background = element_blank())

ggplotly(stackedBar) %>% 
  layout(legend = list(orientation = "h",
                   y = -0.1, x = 0.5))

plot_ly(stackedBarData, 
        x = ~Partei, 
        y = ~count,
        hovertemplate = "my value: %{y}",
        type = "bar", 
        name = ~missing, 
        color = ~missing
        ) %>% 
  layout(yaxis = list(title = "count"), 
         barmode = "stack",
         title = "Verteilung von Twitter Konten pro Partei")
  
```

## Twitter Nutzungsverlauf pro Partei im Jahr 2021
```{r, warning=FALSE, echo=FALSE, message=FALSE, results='hide'}


partei_farben = c(
    "CDU/CSU" = "black",
    "SPD" = "#E30013",
    "FDP" = "#FFDD00",
    "LINKE" = "#BD3075",
    "GRÜNE" = "#19A229",
    "AfD" = "#009FE1",
    "Andere" = "grey"
)

timeline <- ggplot(tweetTimelineData)+
  geom_freqpoly(aes(created_at.x, color = Partei), position = "stack")+ 
  scale_x_datetime(date_breaks = "1 month")+ # Anpassen um die Zeitangabe richtig anzuzeigen
  scale_color_manual(values = partei_farben)+
  labs(
    title = "Tweetsvorkommen der Politiker",
    subtitle = "Zeitverlauf 2021",
    x = "Zeit",
    y = "Anzahl Tweets",
    color = "Retweet"
  )

ggplotly(timeline)
```

## Wordclound der meistgenutzten Tweets in Deutschland pro Partei
```{r, warning=FALSE, echo=FALSE, message=FALSE, results='hide'}
ggplot(topHashtagsLimit, 
       aes(label = hashtags, 
           size = no_hashtags
           )
       ) +
  geom_text_wordcloud_area(area_corr = TRUE, shape = "square", rm_outside = T, area_corr_power = 1) +
  scale_size_area(max_size = 19) +
  theme_minimal() +
  scale_color_gradient(low = "black", high = "lightblue") +
  labs(title = "Die meist verwendeten Hashtags der Parteien") +
  facet_wrap(~Partei)
```

## Deutschlandkarte der Twitternutzung pro Bundesland mit den drei meistgenutzten Hashtags pro Partei
```{r, warning=FALSE, echo=FALSE, message=FALSE, results='hide'}
# References:
# https://cran.r-project.org/web/packages/leaflet.minicharts/vignettes/introduction.html
# https://r-spatial.github.io/mapview/articles/articles/mapview_04-popups.html

cuts <- c(0, 20000, 40000, 60000, 80000, 100000, 120000, 140000, 160000, 180000)
pal <- colorBin("PuBu", domain = de_map$no_tweets, bins = cuts) 

# Beschreibungstext beim Klicken auf das Bundesland
popup <- paste("<strong>Anzahl der Tweets in </strong>", 
                "<strong>", de_map$NAME_1, ": </strong>", "<br>", de_map$no_tweets,
                "<br>", "<br>",
                "<b> Top 3 Hashtags pro Partei: </b>", "<br>",
                de_map$html)

map <- 
  leaflet(data = de_map) %>% 
    addProviderTiles(providers$CartoDB.Voyager) %>% 
  setView(lng = 10.4507147, lat = 50.9833118, zoom = 5) %>% 
  addPolygons(data = de_map, 
              fillColor= ~pal(no_tweets),
              fillOpacity = 0.8, 
              weight = 0.5,
              label = de_map$NAME_1,
              color = "black",
              popup = popup) %>% 
  addLegend("bottomright", 
            pal = pal, 
            values = de_map$no_tweets, 	
            title = "Anzahl der Tweets", 
            opacity = 1)
map
```



