---
title: "R Notebook"
autor: "AlPaMa-DataTeam"
output: html_notebook
---

Describtion about the Project

# Setup:
```{r}
# Packages
packages <- c("ggplot2", "dplyr", "readr", "stringr", "purrr", "janitor", "ggrepel", "rtweet", "twitteR", 'purrr', 'tidyr', 'ggwordcloud', 'htmlwidgets')

# Function to check installation and install packages
pkgTest <- function(x)
{
  if (!require(x,character.only = TRUE))
  {
    install.packages(x,dep=TRUE)
    if(!require(x,character.only = TRUE)) stop("Package not found")
  }
}

# Auto load and install packages
invisible(for (package in packages) {
  pkgTest(package)
  library(package, character.only = TRUE)
})

# Twitter access
api_key <- "KB9450oyqXW9doi7Hb1HlfIfL"
api_secret_key <- "2fhDc53TtMMsHtVugsieYIQWP9T46K2wmPcc7GmgyYxqwQhiTT"
access_token <- "94161327-dWQWONF3UvNPam1fhV10fXCJnIXXinBSCriliCBfP"
access_token_secret <- "ZFA0GSjrTDyIfaASbqgK9Hk775bYkXSyHbdJ2LPtlRHmb"

## authenticate via web browser
token <- create_token(
  app = "AlPaMa - Twitter Analysis",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)
```

# Data:
## Parliamentarians 
```{r}
urlfile = "https://raw.githubusercontent.com/Leibniz-HBI/DBoeS-Automatization/master/db/reviewed/Parlamentarier.csv"
mainData <- read_csv("data/ParlamentarierBackup.csv")
# Try to overwrite up latest data
try(mainData <- read_csv(url(urlfile)))
# Create Backup
write_csv(mainData, "data/ParlamentarierBackup.csv")
```
## Tweets 
```{r}
# Relevant parliamentarians for Twitter scraping
twitterUser = mainData %>% 
  filter(Kategorie != "EU-Parlament") %>% # Switch between EU only and non EU by changing the != to ==
  filter(!is.na(SM_Twitter_id)) %>% 
  mutate(SM_Twitter_id = as.character(SM_Twitter_id))

# Check current rate limits
rate_limit(token) # All
#rate_limit(token, "statuses/user_timeline") # user_timeline
allTweetsRDS = 'data/tweets.rds'
if (file.exists(allTweetsRDS)){
  scrapedTweetsPerUser = readRDS(allTweetsRDS)
} else {
  scrapedTweetsPerUser <- list()
  userList = list(twitterUser$SM_Twitter_user)
  rl <- rate_limit(token, "statuses/user_timeline")
  for(user in userList[[1]]){
    scrapedTweetsPerUser[[user]] <- get_timeline(user, n = 100, check = F)
    print("rate limit remaining:", str(rl$remaining))
    print("at user:", str(user))
    # Artifically Counting down the rate limit
    rl <- rl %>% 
      mutate(remaining = remaining - 1)
    # if rate limit exhausted, then wait to rate limit reset
    if (rl$remaining < 5L) {
      # Get Rate Limits to get time till reset
      rl <- rate_limit(token, "statuses/user_timeline")
      print("rate limit exceeded, waiting for 900s at user", str(user))
      Sys.sleep(as.numeric(rl$reset, "secs"))
      # Get fresh rate limit info after waiting
      rl <- rate_limit(token, "statuses/user_timeline")
    }
  }
  saveRDS(scrapedTweetsPerUser, file = "data/tweets.rds")
}

```
## Data processing
```{r}
# All scraped Tweets in a tidy Dataframe. Modifications: All screen_name Data is lowercase, Added Column with amount of Tweets per user
allScrapedTweets = data.table::rbindlist(scrapedTweetsPerUser) %>% 
  mutate(screen_name = tolower(screen_name)) %>% 
  group_by(screen_name) %>% 
  dplyr::mutate(tweetAmount = n())

# Convert every username to lowercase for correct joining
mainDataLower = mainData %>% 
  mutate(SM_Twitter_user = tolower(SM_Twitter_user))
  
# Joined the dataframes to add all politicans without tweets and set tweetsAmount to 0
mainDataWithTweets = allScrapedTweets %>% 
  full_join(mainDataLower, by = c('screen_name'='SM_Twitter_user')) %>% 
  mutate(tweetAmount = replace_na(tweetAmount, 0))

# Expandet vectors with hashtags to have one hashtag per row (rest of the data got duplicated)
expandedHashtags = mainDataWithTweets %>% 
  unnest(hashtags)
# Amount of tweets per person. Problem with 96 missing entrys (not EU-Parlamentarians)
tweetAmount = mainDataWithTweets %>% 
  filter(Kategorie != "EU-Parlament") %>%
  distinct(Name, .keep_all=TRUE) %>% 
  select(Name, screen_name, tweetAmount)


# Wenn jmd Lust hat kann man hier rausfinden warum 96 Politiker fehlen. Eigentlich müsste der full_join alle in Tweet Amount eingefügt haben
mainDataLowerNames = mainDataLower %>% 
  select(Name)
  
tweetAmountNames = tweetAmount %>% 
  select(Name)

# check for missing rows in two distinct columns
library(plyr)
mainDataLowerNames$match <- ifelse(row.names(mainDataLowerNames) %in% row.names(match_df(mainDataLowerNames,tweetAmountNames)),"yes","no")

missingData = mainDataLowerNames %>% 
  filter(match == "no")


test = mainData %>% 
  filter(Kategorie != "EU-Parlament") %>%
  distinct(Kategorie)

mainDataWithState = mainData %>% 
  mutate(Bundesland, Bundesland = case_when(
    Kategorie == "Abgeordnetenhaus von Berlin"~"Berlin",
    Kategorie == "Bayerischer Landtag"~"Bayern",
    Kategorie == "Bremische Bürgerschaft"~"Bremen",
    Kategorie == "Hamburgische Bürgerschaft"~"Hamburg",
    Kategorie == "Hessischer Landtag"~"Hessen",
    Kategorie == "Landtag Brandenburg"~"Brandenburg",
    Kategorie == "Landtag des Saarlandes"~"Saarland",
    Kategorie == "Landtag Mecklenburg-Vorpommern"~"Mecklenburg-Vorpommern",
    Kategorie == "Landtag Nordrhein-Westfalen"~"Nordrhein-Westfalen",
    Kategorie == "Landtag Rheinland-Pfalz"~"Rheinland-Pfalz",
    Kategorie == "Landtag Sachsen-Anhalt"~"Sachsen-Anhalt",
    Kategorie == "Landtag von Baden-Württemberg"~"Baden-Württemberg",
    Kategorie == "Niedersächsischer Landtag"~"Niedersachsen",
    Kategorie == "Sächsischer Landtag"~"Sachsen",
    Kategorie == "Schleswig-Holsteinischer Landtag"~"Schleswig-Holstein",
    Kategorie == "Thüringer Landtag"~"Thüringen",
    Kategorie == "Bundestag"~"Deutschland",
    TRUE ~ Kategorie))

topHashtags = expandedHashtags %>% 
  select(Partei, hashtags) %>% 
  filter(!is.na(hashtags)) %>% 
  group_by(Partei, hashtags) %>%
  mutate(Partei, Partei = case_when(
    Partei == "CSU"~"CDU",
    Partei == "FDP/DVP"~"FDP",
    Partei == "FW"~"Andere",
    Partei == "fraktionslos"~"Andere",
    TRUE ~ Partei
  )) %>% 
  dplyr::mutate(
    no_hashtags = n()
  ) %>% 
  distinct(Partei, hashtags, no_hashtags, .keep_all = TRUE) %>%
  filter(no_hashtags > 20)
  
top_hashtags = readRDS('data/Top_Hashtags.rds')
ggplot(topHashtags, aes(label = hashtags, size = no_hashtags, color = no_hashtags))+
  geom_text_wordcloud_area(area_corr = TRUE)+
  scale_size_area(max_size = 20) +
  theme_minimal()+
  scale_color_gradient(low = "black", high = "darkblue")+
  labs(
    title = "Die 50 meist verwendeten Hashtags",
    subtitle = "Der aktivsten User zum Thema Super League"
  )+
  facet_wrap(~Partei)
```
Plots:

## Social Media usage of german political paries in 20??
```{r}
### Libraries ###
library(rtweet)
library(dplyr)
library(ggplot2)
library(ggwordcloud)
library(tidyr)

### Twitter-Token ###
token <- get_token()
token

### Top User ###

SuperLeague <- search_tweets("superleague", n = 2000)

User <- SuperLeague %>%
  mutate(
    no_of_tweets = 1
  ) %>% 
  select(user_id)

User <- User %>% 
  group_by(user_id) %>%
  mutate(
    tweets_per_user = n()
  )

User <- User %>% 
  group_by(user_id, tweets_per_user) %>% 
  summarise() %>% 
  arrange(desc(tweets_per_user), .by_group = FALSE)

User2 <- User %>%
  ungroup(user_id) %>% 
  slice(1:100) %>% 
  saveRDS("data/Top_User")

User2 <- readRDS("data/Top_User")

### Timeline ###
UserTL <- get_timelines(User2$user_id, n = 1000, token = token, retryonratelimit = T)

#UserTL <- UserTL %>% 
#  saveRDS("data/Timelines")

UserTL <- readRDS("data/Timelines")

### Hashtags ###

hashtag <- UserTL[,"hashtags", with = FALSE] %>% 
  filter(hashtags != "NA") %>% 
  unnest(cols = c(hashtags)) %>%
  group_by(hashtags) %>% 
  mutate(
    no_hashtags = n()
  )

top_hashtags <- hashtag %>%
  group_by(hashtags, no_hashtags) %>% 
  summarise() %>% 
  arrange(desc(no_hashtags), .by_group = FALSE) %>%
  ungroup(hashtags) %>% 
  slice(1:50)

#top_hashtags <- top_hashtags %>% saveRDS("data/Top_Hashtags")
#top_hashtags <- readRDS("data/Top_Hashtags")
  
### Plot 1 ###

ggplot(SuperLeague)+
  geom_freqpoly(aes(created_at, color = is_retweet), position = "stack", bins = 222)+ #Zeitspanne 18.5h * 60 / 5 = 222
  scale_x_datetime(date_breaks = "4 hours")+
  labs(
    title = "Tweetsvorkommen zur Super League",
    subtitle = "Zeitverlauf 05./06. Mai",
    x = "Zeit",
    y = "Anzahl Tweets",
    color = "Retweet"
    
  )


### Plot 2 ###

ggplot(UserTL)+
  geom_freqpoly(aes(created_at, color = is_retweet), position = "stack")+
  labs(
    title = "Timeline der aktivsten Nutzer",
    subtitle = "Die letzten 1000 Tweets der Nutzer",
    x = "Zeit",
    y = "Anzahl Tweets",
    color = "Retweet"
    
  )

### Plot 3 ###

```


## Word Cloud of commonly used hastags of german german political paries in 2021
```{r}

```



## Twitter Usage per state in 2021
```{r}

```